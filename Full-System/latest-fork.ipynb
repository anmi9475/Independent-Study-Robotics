{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dylan Kriegman / May 2023\n",
    "# modified by Andrea Miller / Fall 2023\n",
    "\n",
    "# usb port for dynamixel: tty.usbmodem14101\n",
    "\n",
    "# idea: failutre recovery << screws example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import open3d as o3d\n",
    "o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Debug)\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2 as cv\n",
    "import scipy.ndimage as nd\n",
    "# Used to display Matplotlib plots in Jupyter\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# PIL used to save images as pngs\"\n",
    "from PIL import Image\n",
    "\n",
    "# Robotics Toolbox used to determine camera coordinate frame given joint angles\n",
    "import roboticstoolbox as rtb\n",
    "\n",
    "# Spatial Math is used for manipulating geometric primitives\n",
    "import spatialmath as sm\n",
    "from spatialmath import SE3\n",
    "\n",
    "# Poses is from rmlib and used for converting between 4 x 4 homogenous pose and 6 element vector representation (x,y,z,rx,ry,rz)\n",
    "import poses\n",
    " \n",
    "import copy\n",
    "\n",
    "import swift\n",
    "\n",
    "from spatialgeometry.geom.CollisionShape import (CollisionShape,Mesh,Cylinder,Cuboid,Box,Sphere)\n",
    "\n",
    "import spatialgeometry as sg\n",
    "\n",
    "# UR Interface\n",
    "import rtde_control\n",
    "import rtde_receive\n",
    "\n",
    "# Gripper Interface \n",
    "from Motor_Code import Motors\n",
    "\n",
    "# For insterfacing with fast downward solver\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UR5_Interface():\n",
    "    def __init__(self):\n",
    "        self.robotIP = \"192.168.0.6\"\n",
    "        # RTDEControlInterface and RTDEReceiveInterface Objects\n",
    "        self.c,self.r = None,None\n",
    "        # Gripper Controller Interface\n",
    "        self.gripperController = None\n",
    "\n",
    "    def getJointAngles(self):\n",
    "        # Returns a 6 element numpy array of joint angles (radians)\n",
    "        thetas = np.array(self.r.getActualQ())\n",
    "        return thetas\n",
    "    \n",
    "    def getPose(self):\n",
    "        # Returns the current pose of the last frame as a SE3 Object (4 x 4 Homegenous Transform)\n",
    "        p = self.r.getActualTCPPose() \n",
    "        poseMatrix = self.poseVectorToMatrix(p)\n",
    "        T_N = sm.SE3(poseMatrix)\n",
    "        # T_N.plot(name=\"C\")\n",
    "        return T_N\n",
    "\n",
    "    \n",
    "    def poseVectorToMatrix(self,poseVector):\n",
    "        # Converts poseVector into an SE3 Object (4 x 4 Homegenous Transform)\n",
    "        # poseVector is a 6 element list of [x, y, z, rX, rY, rZ]\n",
    "        T_N = sm.SE3(poses.pose_vec_to_mtrx(poseVector))\n",
    "        return T_N\n",
    "    \n",
    "    def poseMatrixToVector(self,poseMatrix):\n",
    "        # Converts poseMatrix into a 6 element list of [x, y, z, rX, rY, rZ] \n",
    "        # poseMatrix is a SE3 Object (4 x 4 Homegenous Transform) or numpy array\n",
    "        return poses.pose_mtrx_to_vec(np.array(poseMatrix))\n",
    "        \n",
    "    \n",
    "    def moveJ(self,qGoal):\n",
    "        # qGoal is a 6 element numpy array of joint angles (radians)\n",
    "        # speed is joint velocity (rad/s)\n",
    "        qGoal = list(qGoal)\n",
    "        print(f\"MoveJ to:\\n {np.degrees(qGoal).reshape((6,1))}\")\n",
    "        self.c.moveJ(qGoal,1.05,1.4,True)\n",
    "    \n",
    "    def moveL(self,poseMatrix):\n",
    "        # poseMatrix is a SE3 Object (4 x 4 Homegenous Transform) or numpy array\n",
    "        # Moves tool tip pose linearly in cartesian space to goal pose (requires tool pose to be configured)\n",
    "        # tool pose defined relative to the end of the gripper when closed\n",
    "        poseVector = self.poseMatrixToVector(poseMatrix)\n",
    "        self.c.moveL(poseVector, 0.25, 0.5, False)\n",
    "        \n",
    "        \n",
    "    def moveHome(self):\n",
    "        # Moves the arm linearly in cartesian space to home pose\n",
    "        homePose = np.array([[ 0.99955322, -0.02418213, -0.01756664,  0.01498893],\n",
    "                             [-0.01748495,0.00358545,-0.9998407,-0.57686779],\n",
    "                             [0.02424126,0.99970114,0.00316103,0.05545535],\n",
    "                             [0,0,0,1]])\n",
    "        self.arm.move(target=homePose,move_type=\"l\")\n",
    "    \n",
    "    def openGripper(self):\n",
    "        self.gripperController.openGripper()\n",
    "        \n",
    "    def closeGripper(self,width=10):\n",
    "        # Computes the servo angles needed for the jaws to be width mm apart\n",
    "        # Sends command over serial to the gripper to hold those angles\n",
    "        dTheta = self.gripperController.distance2theta(width)\n",
    "        self.gripperController.position(dTheta)\n",
    "        \n",
    "        \n",
    "    def testRoutine(self):\n",
    "        # Moves ur +1 cm in world frame z-axis then down 1 cm and then opens, closes, and opens gripper  \n",
    "        print(\"Running Test Routine\")\n",
    "        initPose = np.array(self.getPose())\n",
    "        # print(f\"TCP Pose:\\n{initPose}\")\n",
    "        dX,dY,dZ = 0,0,2/100 # in m\n",
    "        goalPose = initPose\n",
    "        goalPose[2][3] += dZ\n",
    "        goalPose = sm.SE3(goalPose)\n",
    "        # print(f\"Goal TCP Pose:\\n{goalPose}\")\n",
    "        print(\"Running UR Test\")\n",
    "        self.moveL(sm.SE3(goalPose))\n",
    "        # print(f\"Final TCP Pose:\\n{self.getPose()}\")\n",
    "        goalPose = np.array(self.getPose())\n",
    "        goalPose[2][3] -= dZ\n",
    "        goalPose = sm.SE3(goalPose)\n",
    "        self.moveL(goalPose)\n",
    "        print(\"Running Gripper Test\")\n",
    "        # print(\"Opening Gripper\")\n",
    "        self.openGripper()\n",
    "        time.sleep(1)\n",
    "        self.closeGripper(10)\n",
    "        time.sleep(2)\n",
    "        self.openGripper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cu.URT0', 'cu.DraysPowerbeatsPro', 'cu.DraysBeatsStudio', 'cu.Bluetooth-Incoming-Port', 'cu.usbmodem14101']\n"
     ]
    }
   ],
   "source": [
    "import serial.tools.list_ports\n",
    "ports = []\n",
    "for port in serial.tools.list_ports.comports():\n",
    "    ports.append(port.name)\n",
    "print(ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded to open the port\n",
      "Succeeded to change the baudrate\n",
      "Moving speed of dxl ID: 1 set to 100 \n",
      "Moving speed of dxl ID: 2 set to 100 \n"
     ]
    }
   ],
   "source": [
    "# Call this once to intialize serial connections to ur and gripper\n",
    "# To list serial ports of the motor interface\n",
    "# $ python -m serial.tools.list_ports\n",
    "\n",
    "\n",
    "robotIP = \"192.168.0.6\"\n",
    "con = rtde_control.RTDEControlInterface(robotIP)\n",
    "rec = rtde_receive.RTDEReceiveInterface(robotIP)\n",
    "\n",
    "# original: servoPort = \"/dev/ttyACM0\"\n",
    "# use 'ls /dev' in terminal to find port address being used by dynamixel motor\n",
    "servoPort = \"/dev/tty.usbmodem14101\"\n",
    "\n",
    "# original: gripperController = Motors(servoPort)\n",
    "gripperController = Motors(servoPort)\n",
    "\n",
    "gripperController.torquelimit(600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# con.disconnect()\n",
    "# rec.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test Routine\n",
      "Running UR Test\n",
      "Running Gripper Test\n",
      "Position of dxl ID: 1 set to 303 \n",
      "Position of dxl ID: 2 set to 729 \n",
      "Position of dxl ID: 1 set to 549 \n",
      "Position of dxl ID: 2 set to 490 \n",
      "Position of dxl ID: 1 set to 303 \n",
      "Position of dxl ID: 2 set to 729 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    ur = UR5_Interface()\n",
    "    ur.c,ur.r,ur.gripperController = con,rec,gripperController\n",
    "    ur.testRoutine()\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pass\n",
    "    # ur.c.disconnect()\n",
    "    # ur.r.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RTB_Model():\n",
    "    # Kinematic Model of the Robot in the Robotics Toolbox for Python (RTB)\n",
    "    # Not really needed but helpful for visualization / testing\n",
    "    def __init__(self):\n",
    "        # Model has units in meters\n",
    "        self.ur5_DH = rtb.models.DH.UR5() # URDF model\n",
    "        self.ur5_URDF = rtb.models.UR5() # DH parameter-based model \n",
    "\n",
    "        homeJointAngles = np.array(np.radians([53,-112,144,-27.5,55,171.7]))\n",
    "        self.setJointAngles(homeJointAngles)\n",
    "        \n",
    "    def setJointAngles(self,thetas):\n",
    "        # thetas: 6 x 1 numpy array of joint angles (radians)\n",
    "        self.ur5_DH.q = thetas\n",
    "        self.ur5_URDF.q = thetas\n",
    "        \n",
    "    def initSwiftEnv(self):\n",
    "        self.swiftEnv = swift.Swift()\n",
    "        self.swiftEnv.launch()\n",
    "        self.swiftEnv.add(self.ur5_URDF)\n",
    "        endEffectorFrame = self.ur5_DH.fkine_all(self.ur5_DH.q)[-1] * SE3()\n",
    "        endEffectorFrame.t[0:2] *= -1\n",
    "        axes = sg.Axes(length=0.5,pose=endEffectorFrame)\n",
    "        self.swiftEnv.add(axes)\n",
    "        \n",
    "    def addSwiftBox(self,pos,dims=(0.01905,0.01905,0.01905)):\n",
    "        # Swift Model seems to have a different coordinate system than the DH model\n",
    "        # The x-axis and y-axis point in the opposite direction as the DH model\n",
    "        # pos is tuple of (x,y,z)\n",
    "        # dims is tuple of (length,width,height)\n",
    "        box = Cuboid(dims,pose=SE3(-pos[0],-pos[1],pos[2]))\n",
    "        self.swiftEnv.add(box)\n",
    "\n",
    "    def simulateSwiftRobot(self):\n",
    "        T = np.eye(4)\n",
    "        T[0:3,3] = np.array([0,0.25,0.25]).T\n",
    "        print(T)\n",
    "        \n",
    "        # put a breakpoint here and look at the object to see what it is\n",
    "        # \n",
    "        sol = self.ur5_URDF.ikine_LM(SE3(T)) # Solve IK to get fgoal pose\n",
    "        print(sol)\n",
    "        qtraj = rtb.jtraj(self.ur5_URDF.q,sol.q,50)\n",
    "        dt = 0.05\n",
    "        self.swiftEnv.start_recording(\"ur5\",1/dt)\n",
    "        for qk in qtraj.q:\n",
    "            print(qk)\n",
    "            self.setJointAngles(qk) # update robot state\n",
    "            self.swiftEnv.step(dt) # update visualization\n",
    "        self.swiftEnv.stop_recording()\n",
    "        # dt = 0.050\n",
    "    \n",
    "    def plotRobot(self):\n",
    "        # Displays DH robot in matplotlib\n",
    "        ur5 = self.ur5_DH\n",
    "        env = ur5.plot(ur5.q) # PyPlot backend\n",
    "\n",
    "        \n",
    "        # env.ax.scatter([0,0],[0,0],[0.4,0.45])\n",
    "        # T_C = self.getCameraFrame()\n",
    "        # T_C.plot(frame=\"C\",length=0.1)\n",
    "        # env.hold()\n",
    "    \n",
    "    def getGripperPose(self):\n",
    "        # Returns the pose (4 x 4 Homogenous Transform as SE3 Spatial Math Object) with position in the center between the 2 gripper links\n",
    "        ur5 = self.ur5_DH\n",
    "        T_N = ur5.fkine_all(ur5.q)[-1] # T_N - end-effector frame (before optoforce/gripper)\n",
    "        d = 0.1125 # distance between end-effector frame origin and center of gripper frame along z-axis (m)\n",
    "        T_G = T_N * SE3.Tz(d)\n",
    "        return T_G\n",
    "    \n",
    "    def getCameraFrame(self):\n",
    "        # Robot joint angles need to be set pior <<<<< NEED TO BE SET TO WHAT?\n",
    "        # Returns a SE3 Spatial Math Object (4 x 4 Homogenous Transform) corresponding to the robot's camera frame \n",
    "        ur5 = self.ur5_DH\n",
    "        T_N = ur5.fkine_all(ur5.q)[-1] # T_N - end-effector frame (before optoforce/gripper)\n",
    "        d = 0.292 # distance between end-effector frame origin and center of camera frame along z-axis (m)\n",
    "        P_C = np.array([0,0,d]) # Translation from frame T_N to origin of camera frame (m)\n",
    "        theta = np.radians(90) # Rotation about the Z-axis between the camera frame and end-effector frame\n",
    "        T_C = T_N * SE3.Tz(d) * SE3.Rz(theta) # Camera coordinate frame\n",
    "        return T_C\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.25]\n",
      " [0.   0.   1.   0.25]\n",
      " [0.   0.   0.   1.  ]]\n",
      "IKSolution: q=[-0.8252, -1.279, -2.259, -2.745, 0.7456, 3.142], success=True, iterations=15, searches=1, residual=2.92e-12\n",
      "[ 0.9250245  -1.95476876  2.51327412 -0.47996554  0.95993109  2.99673033]\n",
      "[ 0.92488026 -1.95471306  2.51288078 -0.48015222  0.95991343  2.99674226]\n",
      "[ 0.92390608 -1.95433686  2.51022426 -0.48141296  0.95979415  2.9968229 ]\n",
      "[ 0.92136776 -1.95335664  2.50330244 -0.48469794  0.95948337  2.997033  ]\n",
      "[ 0.91663145 -1.95152762  2.49038691 -0.49082744  0.95890347  2.99742502]\n",
      "[ 0.90915925 -1.94864209  2.47001077 -0.50049761  0.95798861  2.9980435 ]\n",
      "[ 0.89850467 -1.94452761  2.44095653 -0.51428627  0.9566841   2.99892538]\n",
      "[ 0.88430824 -1.93904538  2.40224392 -0.53265863  0.95494594  3.00010042]\n",
      "[ 0.86629299 -1.93208844  2.35311768 -0.55597312  0.95274022  3.00159155]\n",
      "[ 0.84426004 -1.92357999  2.29303548 -0.58448713  0.95004259  3.00341522]\n",
      "[ 0.81808411 -1.91347164  2.22165568 -0.6183628   0.9468377   3.0055818 ]\n",
      "[ 0.78770907 -1.90174173  2.13882522 -0.65767276  0.94311869  3.00809595]\n",
      "[ 0.75314346 -1.88839354  2.0445674  -0.70240595  0.93888661  3.01095696]\n",
      "[ 0.71445606 -1.87345365  1.93906976 -0.75247337  0.93414987  3.01415912]\n",
      "[ 0.67177141 -1.85697014  1.82267192 -0.80771385  0.92892372  3.01769214]\n",
      "[ 0.62526534 -1.83901091  1.69585336 -0.86789982  0.92322969  3.02154146]\n",
      "[ 0.57516054 -1.81966197  1.55922132 -0.93274311  0.91709504  3.02568864]\n",
      "[ 0.52172206 -1.79902566  1.41349859 -1.00190069  0.91055223  3.03011175]\n",
      "[ 0.46525288 -1.77721898  1.25951136 -1.07498046  0.90363835  3.03478572]\n",
      "[ 0.40608943 -1.75437186  1.09817708 -1.15154703  0.8963946   3.03968269]\n",
      "[ 0.34459715 -1.73062542  0.93049223 -1.23112746  0.88886571  3.04477242]\n",
      "[ 0.281166   -1.70613024  0.75752024 -1.31321708  0.88109944  3.05002263]\n",
      "[ 0.21620602 -1.68104468  0.58037926 -1.39728525  0.87314598  3.05539938]\n",
      "[ 0.15014288 -1.6555331   0.40023001 -1.48278108  0.86505745  3.06086744]\n",
      "[ 0.08341337 -1.6297642   0.21826363 -1.56913929  0.85688733  3.06639066]\n",
      "[ 0.01646099 -1.60390924  0.03568952 -1.65578592  0.84868993  3.07193232]\n",
      "[-0.05026852 -1.57814033 -0.14627685 -1.74214413  0.84051982  3.07745554]\n",
      "[-0.11633167 -1.55262876 -0.3264261  -1.82763996  0.83243129  3.0829236 ]\n",
      "[-0.18129164 -1.5275432  -0.50356709 -1.91170812  0.82447783  3.08830035]\n",
      "[-0.24472279 -1.50304802 -0.67653908 -1.99379775  0.81671156  3.09355056]\n",
      "[-0.30621508 -1.47930158 -0.84422392 -2.07337818  0.80918267  3.09864029]\n",
      "[-0.36537852 -1.45645446 -1.00555821 -2.14994475  0.80193892  3.10353726]\n",
      "[-0.4218477  -1.43464778 -1.15954543 -2.22302452  0.79502504  3.10821122]\n",
      "[-0.47528618 -1.41401147 -1.30526816 -2.2921821   0.78848223  3.11263434]\n",
      "[-0.52539099 -1.39466252 -1.4419002  -2.35702539  0.78234758  3.11678152]\n",
      "[-0.57189705 -1.3767033  -1.56871876 -2.41721136  0.77665355  3.12063084]\n",
      "[-0.6145817  -1.36021979 -1.68511661 -2.47245184  0.7714274   3.12416386]\n",
      "[-0.6532691  -1.34527989 -1.79061424 -2.52251926  0.76669066  3.12736602]\n",
      "[-0.68783471 -1.33193171 -1.88487206 -2.56725245  0.76245857  3.13022703]\n",
      "[-0.71820975 -1.3202018  -1.96770253 -2.60656241  0.75873957  3.13274117]\n",
      "[-0.74438568 -1.31009345 -2.03908232 -2.64043807  0.75553468  3.13490776]\n",
      "[-0.76641863 -1.301585   -2.09916453 -2.66895209  0.75283705  3.13673143]\n",
      "[-0.78443388 -1.29462806 -2.14829076 -2.69226658  0.75063133  3.13822256]\n",
      "[-0.79863032 -1.28914583 -2.18700338 -2.71063894  0.74889317  3.1393976 ]\n",
      "[-0.8092849  -1.28503135 -2.21605761 -2.7244276   0.74758866  3.14027948]\n",
      "[-0.8167571  -1.28214582 -2.23643375 -2.73409777  0.74667379  3.14089796]\n",
      "[-0.8214934  -1.2803168  -2.24934929 -2.74022727  0.7460939   3.14128998]\n",
      "[-0.82403172 -1.27933658 -2.2562711  -2.74351225  0.74578311  3.14150008]\n",
      "[-0.8250059  -1.27896038 -2.25892762 -2.74477299  0.74566384  3.14158071]\n",
      "[-0.82515015 -1.27890468 -2.25932097 -2.74495966  0.74564618  3.14159265]\n"
     ]
    }
   ],
   "source": [
    "# unable to run last line, ''UR5' object has no attribute 'ikine_LMS' -Andrea\n",
    "\n",
    "r = RTB_Model()\n",
    "r.getCameraFrame()\n",
    "r.initSwiftEnv()\n",
    "r.addSwiftBox([0,0,0.5])\n",
    "r.simulateSwiftRobot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RealSense():\n",
    "    def __init__(self,zMax = 0.5,voxelSize = 0.001):\n",
    "        self.pinholeIntrinsics = None # set in self.takeImages()\n",
    "        self.zMax = zMax # max distance for objects in depth images (m)\n",
    "        # downsample point cloud with voxel size = 1 mm (0.001 m / 0.04 in)\n",
    "        self.voxelSize = voxelSize\n",
    "        self.pcd = o3d.geometry.PointCloud() # current pcd from realsense\n",
    "        self.extrinsics = np.eye(4) # extrinsic parameters of the camera frame 4 x 4 numpy array\n",
    "        self.cameraFrameTransform = np.eye(4)\n",
    "        self.pipe,self.config = None,None\n",
    "    \n",
    "    def initConnection(self):\n",
    "        # Initializes connection to realsense, sets pipe,config values\n",
    "        self.pipe = rs.pipeline()\n",
    "        self.config = rs.config()\n",
    "\n",
    "        # Getting information about the connected realsense model (device object) - D405\n",
    "        pipeProfile = self.config.resolve(rs.pipeline_wrapper(self.pipe))\n",
    "        device = pipeProfile.get_device()\n",
    "        depth_sensor = device.first_depth_sensor()\n",
    "        self.depthScale = depth_sensor.get_depth_scale()\n",
    "        # print(depth_scale)\n",
    "\n",
    "        # 1 - default, 2 - hand, 3 - high accuracy, 4 - high density, 5 - medium density\n",
    "        depth_sensor.set_option(rs.option.visual_preset,4) # 4 corresponds to high-density option\n",
    "\n",
    "        # Setting attributes for stream\n",
    "        # Depth Stream (1280 x 720) 5 fps - D405 Sensor has max 1280 x 720\n",
    "        # (Minimum z depth is between 55-70 mm)\n",
    "        self.config.enable_stream(rs.stream.depth,1280,720,rs.format.z16,5)\n",
    "\n",
    "        # Color and Infrared D405 Streams Available (1280 x 720) 5 fps - D405 Sensor has max 1280 x 720\n",
    "        self.config.enable_stream(rs.stream.color,1280,720,rs.format.rgb8,5)\n",
    "        \n",
    "        # Starting the pipeline based on the specified configuration\n",
    "        self.pipe.start(self.config)\n",
    "        \n",
    "    def getPinholeInstrinsics(self,frame):\n",
    "        # frame is a subclass of pyrealsense2.video_frame (depth_frame,etc)\n",
    "        intrinsics = frame.profile.as_video_stream_profile().intrinsics\n",
    "        return o3d.camera.PinholeCameraIntrinsic(intrinsics.width,intrinsics.height, intrinsics.fx,\n",
    "                                                intrinsics.fy, intrinsics.ppx,\n",
    "                                                intrinsics.ppy)\n",
    "\n",
    "\n",
    "    def takeImages(self,save=False):\n",
    "        # Takes RGBD Image using Realsense\n",
    "        # intrinsic and extrinsic parameters are NOT applied only in getPCD()\n",
    "        # out: Open3D RGBDImage\n",
    "        pipe,config = self.pipe,self.config\n",
    "        \n",
    "        frames = pipe.wait_for_frames()\n",
    "        depthFrame = frames.get_depth_frame() # pyrealsense2.depth_frame\n",
    "        colorFrame = frames.get_color_frame()\n",
    "\n",
    "        # Sets class value for intrinsic pinhole parameters\n",
    "        self.pinholeInstrinsics = self.getPinholeInstrinsics(colorFrame)\n",
    "        # asign extrinsics here if the camera pose is known\n",
    "        # alignOperator maps depth frames to color frames\n",
    "        alignOperator = rs.align(rs.stream.color)\n",
    "        alignOperator.process(frames)\n",
    "        alignedDepthFrame,alignedColorFrame = frames.get_depth_frame(),frames.get_color_frame()\n",
    "        \n",
    "        # unmodified rgb and z images as numpy arrays of 3 and 1 channels\n",
    "        rawColorImage = np.array(alignedColorFrame.get_data())\n",
    "        rawDepthImage = np.asarray(alignedDepthFrame.get_data())\n",
    "\n",
    "        rawRGBDImage = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            o3d.geometry.Image(rawColorImage),\n",
    "            o3d.geometry.Image(rawDepthImage.astype('uint16')),\n",
    "            depth_scale=1.0 / self.depthScale,\n",
    "            depth_trunc = self.zMax,\n",
    "            convert_rgb_to_intensity=False)\n",
    "        \n",
    "        if save:\n",
    "            subFix = str(time.time()) \n",
    "            np.save(f\"depthImage{subFix}\",rawRGBDImage.depth)\n",
    "            np.save(f\"colorImage{subFix}\",rawRGBDImage.color)\n",
    "            colorIM = Image.fromarray(rawColorImage)\n",
    "            colorIM.save(f\"colorImage{subFix}.jpeg\")\n",
    "        return rawRGBDImage\n",
    "\n",
    "    def getPCD(self,save=False):\n",
    "        # Takes images and returns a PCD and RGBD Image\n",
    "        # Applies extrinsics and zMax\n",
    "        # Downsamples PCD based on self.voxelSize\n",
    "        # :save boolean that toggles whether to save data\n",
    "        # out: tuple of (open3d point cloud (o3d.geometry.PointCloud),RGBDImage)\n",
    "        rawRGBDImage = self.takeImages()\n",
    "        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "            rawRGBDImage,\n",
    "            self.pinholeInstrinsics,\n",
    "            project_valid_depth_only=True,\n",
    "            extrinsic = self.extrinsics\n",
    "        ) \n",
    "\n",
    "        # Don't downsample\n",
    "        # downsampledPCD = pcd.voxel_down_sample(voxel_size=self.voxelSize)\n",
    "        if save:\n",
    "            subFix = time.time()\n",
    "            np.save(f\"colorImage{subFix}\",np.array(rawRGBDImage.color))\n",
    "            np.save(f\"depthImage{subFix}\",np.array(rawRGBDImage.depth))\n",
    "            o3d.io.write_point_cloud(f\"pcd{subFix}.pcd\",downsampledPCD)\n",
    "        return pcd,rawRGBDImage\n",
    "        #return (downsampledPCD,rawRGBDImage)\n",
    "\n",
    "    def displayImages(self,depthImg,colorImg):\n",
    "        # Displays a depth and color image given the rgbdImage\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"RealSense Color Image\")\n",
    "        plt.imshow(depthImg)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"RealSense Depth Image\")\n",
    "        plt.imshow(colorImg)\n",
    "        plt.show()\n",
    "    \n",
    "    def displayPCD(self,pcds):\n",
    "        # Displays a list of point clouds given an array of pcd's. Displays camera frame if self.extrinsics != None\n",
    "        # flip_transform = [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]]\n",
    "        # pcd.transform(flip_transform)\n",
    "        worldFrame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.075,origin=[0,0,0])\n",
    "        if (self.extrinsics is None) == False:\n",
    "            cameraFrame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.075)\n",
    "            cameraFrame.transform(self.cameraFrameTransform)\n",
    "            res = [worldFrame,cameraFrame]\n",
    "            res.extend(pcds)\n",
    "            baseSphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.025)\n",
    "            res.append(baseSphere)\n",
    "            o3d.visualization.draw_geometries(res)\n",
    "        else:\n",
    "            res = [worldFrame].extend(pcds)\n",
    "            o3d.visualization.draw_geometries(res)\n",
    "    \n",
    "    def displayStream(self):\n",
    "        # streams and displays the point cloud data in open3d\n",
    "        # pipe,config are stream properties set in the earlier cells \n",
    "        # Streaming loop\n",
    "        pipe,config = self.pipe,self.config\n",
    "        vis = o3d.visualization.Visualizer()\n",
    "        vis.create_window()\n",
    "        framesTaken = 0\n",
    "        displayedPCD = o3d.geometry.PointCloud()\n",
    "        try:\n",
    "            while True:\n",
    "                temp = self.getPCD()[0]\n",
    "                displayedPCD.points = temp.points\n",
    "                displayedPCD.colors = temp.colors\n",
    "                if framesTaken == 0:\n",
    "                    vis.add_geometry(displayedPCD)\n",
    "                    worldFrame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.25,origin=[0,0,0])\n",
    "                    vis.add_geometry(worldFrame)\n",
    "                vis.update_geometry(displayedPCD)\n",
    "                framesTaken += 1\n",
    "                t0 = time.time()\n",
    "                vis.poll_events()\n",
    "                vis.update_renderer()\n",
    "                #time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\"Stream Issue. Exception Raised\")\n",
    "            # raise(e)\n",
    "        # closes window when cell is stopped (exception raised)\n",
    "        finally:\n",
    "            vis.destroy_window()\n",
    "            # pipe.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No device connected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mpipeline()\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mconfig()\n\u001b[0;32m----> 3\u001b[0m profile \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No device connected"
     ]
    }
   ],
   "source": [
    "pipe = rs.pipeline()\n",
    "config = rs.config()\n",
    "profile = pipe.start()\n",
    "try:\n",
    "  for i in range(0, 100):\n",
    "    frames = pipe.wait_for_frames()\n",
    "    for f in frames:\n",
    "      print(f.profile)\n",
    "finally:\n",
    "    pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No device connected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# robot_model = RTB_Model()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m real \u001b[38;5;241m=\u001b[39m RealSense()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mreal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# real.getPCD(True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# real.extrinsics = np.array(robot_model.getCameraFrame())\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(real.extrinsics)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m pcd,rgbdImage \u001b[38;5;241m=\u001b[39m real\u001b[38;5;241m.\u001b[39mgetPCD(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36mRealSense.initConnection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mconfig()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Getting information about the connected realsense model (device object) - D405\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m pipeProfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m device \u001b[38;5;241m=\u001b[39m pipeProfile\u001b[38;5;241m.\u001b[39mget_device()\n\u001b[1;32m     20\u001b[0m depth_sensor \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mfirst_depth_sensor()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No device connected"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RTDEControlInterface: Could not receive data from robot...\n",
      "RTDEControlInterface Exception: Operation canceled [system:89 at /usr/local/include/boost/asio/detail/reactive_socket_recv_op.hpp:134:37 in function 'do_complete']\n",
      "RTDEControlInterface: Robot is disconnected, reconnecting...\n",
      "RTDEReceiveInterface Exception: Operation canceled [system:89 at /usr/local/include/boost/asio/detail/reactive_socket_recv_op.hpp:134:37 in function 'do_complete']\n",
      "RTDEControlInterface Exception: Timeout connecting to UR dashboard server.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconnecting...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# robot_model = RTB_Model()\n",
    "real = RealSense()\n",
    "real.initConnection()\n",
    "# real.getPCD(True)\n",
    "# real.extrinsics = np.array(robot_model.getCameraFrame())\n",
    "# print(real.extrinsics)\n",
    "pcd,rgbdImage = real.getPCD(True)\n",
    "depthImage,colorImage = rgbdImage.depth,rgbdImage.color\n",
    "real.displayImages(colorImage,depthImage)\n",
    "# real.displayPCD(pcd)\n",
    "real.pipe.stop()\n",
    "\n",
    "# o3d.visualization.draw()\n",
    "# r.displayStream()\n",
    "# finally:\n",
    "#     r.pipe.stop()\n",
    "# profile = pipe.start(config)\n",
    "# depth_sensor = profile.get_device().first_depth_sensor()\n",
    "# print(\n",
    "# r.displayStream(pipe,config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# real = RealSense()\n",
    "# real.initConnection()\n",
    "# real.displayStream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "robot_model = RTB_Model()\n",
    "real = RealSense()\n",
    "real.extrinsics = np.array(robot_model.getCameraFrame())\n",
    "print(real.extrinsics)\n",
    "pcd = o3d.io.read_point_cloud(\"pcd1.pcd\")\n",
    "real.displayPCD(pcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block():\n",
    "    def __init__(self,name,pcd,urPose):\n",
    "        # :name string that is block name in PDDL\n",
    "        # :pcd Open3D Point Cloud that contains only the block\n",
    "        # :urPose 4x4 Matrix or SE3 Transform that is the current pose of the Nth frame of the UR5 (given by ur.getPose) when the image was taken\n",
    "        self.blockPCD = pcd\n",
    "        self.name = name\n",
    "        # Removes outlier points by fitting block into largest cluster\n",
    "        self.clusterBlockPCD()\n",
    "        self.blockAABB = self.blockPCD.get_axis_aligned_bounding_box()\n",
    "        self.blockOBB = self.blockPCD.get_oriented_bounding_box()\n",
    "        self.blockAABB.color,self.blockOBB.color = [0,0,0],[0,0,0]\n",
    "        self.urPose = urPose # Pose of the Nth frame of the UR5 when the image was taken\n",
    "        x,y = self.blockAABB.get_center()[0:2]\n",
    "        # due to convex hull outliers are included when mask is off. Use min bound rather than center\n",
    "        zMin = self.blockAABB.get_min_bound()[2]  \n",
    "        self.camFrameCoords = np.matrix([x,y,zMin])\n",
    "        self.gripperFrameCoords = self.getCenterInGripperFrame()\n",
    "        self.worldFrameCoords = self.getCenterInWorld() # Approximate coordinates in world frame \n",
    "        \n",
    "    def clusterBlockPCD(self):\n",
    "        # modifies block PCD to only contain points in the largest cluster found with DBScan\n",
    "        # eps found experimentally\n",
    "        with o3d.utility.VerbosityContextManager(\n",
    "            o3d.utility.VerbosityLevel.Error) as cm:\n",
    "                # eps is radius\n",
    "                # rejects points that are too small\n",
    "                labels = np.array(self.blockPCD.cluster_dbscan(eps=0.005, min_points=20, print_progress=False))\n",
    "\n",
    "        max_label = labels.max()\n",
    "        colors = plt.get_cmap(\"tab20\")(labels / (max_label if max_label > 0 else 1))\n",
    "        colors[labels < 0] = 0\n",
    "        clusters = {}\n",
    "        for i in range(0,max_label + 1):\n",
    "            clusters[i] = []\n",
    "\n",
    "        for i in range(0,len(labels)):\n",
    "            if labels[i] != -1:\n",
    "                clusters[labels[i]].append(i)\n",
    "\n",
    "        clusterPCDs = [] # store pcd and number of points\n",
    "        for clusterLabel in clusters:\n",
    "            clusterPCD = self.blockPCD.select_by_index(clusters[clusterLabel])\n",
    "            clusterPCDs.append((len(clusterPCD.points),clusterPCD))\n",
    "        \n",
    "        print(clusterPCDs)\n",
    "        clusterPCDs.sort()\n",
    "        clusterPCDs.reverse()\n",
    "        \n",
    "        self.blockPCD = clusterPCDs[0][1]\n",
    "    \n",
    "    \n",
    "    def getCenterInGripperFrame(self):\n",
    "        # returns the center of the block in the gripper frame given PCD with no extrinsics applied\n",
    "        R = np.array([[0,1,0],\n",
    "                      [-1,0,0],\n",
    "                      [0,0,1]]) # camera frame basis with respect to gripper frame\n",
    "        t = np.array([0,9,59.3]) / 1000  # camera frame origin with respect to gripper frame (mm)\n",
    "        # Homogenous coordinates         \n",
    "        # gripperFrameCoords = np.matmul(np.array(sm.SE3.Rt(R,t)),self.cameraFrameCoords[0:3])\n",
    "        gripperFrameCoords = (sm.SE3.Rt(R,t) * sm.SE3.Trans(self.camFrameCoords[0:3])).t\n",
    "        '''\n",
    "        # For visualization in displayPCD\n",
    "        self.real.cameraFrameTransform = np.array(camera_frame_transform)\n",
    "        print(f\"Camera Frame Transform:\\n{self.real.cameraFrameTransform}\")\n",
    "        self.real.extrinsics = np.array(camera_frame_transform.inv())\n",
    "        print(f\"Extrinsics:\\n{self.real.extrinsics}\")\n",
    "        '''\n",
    "        \n",
    "        return gripperFrameCoords\n",
    "\n",
    "    def getCenterInWorld(self):\n",
    "        # :urPose SE3 Transform that is the current pose of the Nth frame of the UR5 (given by ur.getPose)\n",
    "        # Uses pose of the Nth frame from UR5 Interface to return the approximate center of the block in the world frame\n",
    "        # may be wrong due to innacurate measurements of end-effector tool dimensions\n",
    "        # CHANGE CURRENT POSE WHEN CONNECTED TO BOT\n",
    "        currentPose = self.urPose # Pose of Nth frame of UR5, SE3 Object\n",
    "        # d should probably be a 3D translation but this is for testing\n",
    "        d = 0.1125 # estimated distance between origin of Nth link and and center of gripper frame along urPose's z-axis (m)\n",
    "        gripperFramePose = currentPose * sm.SE3.Tz(d)\n",
    "        worldFrameCoords = (gripperFramePose * sm.SE3.Trans(self.gripperFrameCoords)).t\n",
    "        return worldFrameCoords\n",
    "   \n",
    "    def getWorldFrameVerticalInGripper(self,verticalDist):\n",
    "        # given a displacement verticalDist along the z-axis in the world frame, determine the same displacement in the gripper frame\n",
    "        # used to find the position directly above the blocks in the gripper frame\n",
    "        return np.matmul(self.urPose.inv().R,(sm.SE3.Trans([0,0,verticalDist]).t - self.urPose.t))\n",
    "    \n",
    "\n",
    "\n",
    "    def getGraspPoint(self):\n",
    "        # returns the (x,y,z) coordinates in either the world or camera coordinate frame of where the gripper should be placed (depending on if extrinsics were set when creating the PCD)\n",
    "        # center of front-facing axis-aligned bounding box\n",
    "        x,y,z = self.blockAABB.get_center()[0:3]\n",
    "        # z = self.blockAABB.get_min_bound()[2]        \n",
    "        return (x,y,z)\n",
    "    \n",
    "    def move(self,goalCamCoords):\n",
    "        self.currentCamCoords = goalCamCoords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "class ObjectDetection():\n",
    "    # This class creates a RealSense Object, takes images and returns Open3D point clouds corresponding to blocks\n",
    "    # Extrinsics of RealSense Object are no longer used here so interfacing with the Realsense could be done outside this class for decoupling\n",
    "    # Additionally, migration of world and gripper frame position to block objects means moveRelative can also be removed\n",
    "    def __init__(self,RealSense,robot_model,moveRelative = True):\n",
    "        # :RealSense - RealSense object\n",
    "        # :moveRelative boolean - True if the gripper moves to block positions in the camera frame, false if moving to world frame positions (via rtb_model)\n",
    "        # robot_model is object of type RTB_Model\n",
    "        self.real = RealSense\n",
    "        '''\n",
    "        # self.real.initConnection()\n",
    "        if moveRelative:\n",
    "            t = np.array([0,9,59.3]) / 1000\n",
    "            R = np.array([[0,1,0],[-1,0,0],[0,0,1]])\n",
    "            camera_frame_transform = sm.SE3.Rt(R,t)\n",
    "            self.real.cameraFrameTransform = np.array(camera_frame_transform)\n",
    "            # print(f\"Camera Frame Transform:\\n{self.real.cameraFrameTransform}\")\n",
    "            self.real.extrinsics = np.array(camera_frame_transform.inv())\n",
    "            # print(f\"Extrinsics:\\n{self.real.extrinsics}\")\n",
    "        else:\n",
    "            T_C = robot_model.getCameraFrame()\n",
    "            print(T_C)\n",
    "            self.real.cameraFrameTransform = np.array(T_C)\n",
    "            self.real.extrinsics = np.array(T_C.inv())\n",
    "        '''\n",
    "        # Load the model into memory\n",
    "        # Trained on yolov8l-seg for 200 epochs\n",
    "        # yolo models compared https://docs.ultralytics.com/tasks/segment/\n",
    "        self.model = YOLO('yolov8l-seg.pt')\n",
    "\n",
    "    def getSegmentationMask(self,result,className):\n",
    "        # :result ultralytics.result\n",
    "        # :className string corresponding to label in trained YOLO model\n",
    "        # Here, className should be in {'Red','Yellow','Blue'}\n",
    "        # Returns 1st instance of the class as binary numpy array and None if the class is not present\n",
    "        classList = list(np.array(result.boxes.cls))\n",
    "        for i in range(0,len(classList)):\n",
    "            predictedClassName = result.names[classList[i]]\n",
    "            if predictedClassName == className:\n",
    "                mask = result.masks.data[i].numpy() # (384,640)\n",
    "                # Resize mask to original imae size\n",
    "                scaledMask = cv2.resize(mask,(result.orig_shape[1],result.orig_shape[0]))\n",
    "                return scaledMask\n",
    "        return None\n",
    "    \n",
    "    def getBlocksFromImages(self,colorImage,depthImage,urPose,display = False):\n",
    "        # :colorImage 3-channel rgb image as numpy array\n",
    "        # :depthImage 1-channel of measurements in z-axis as numpy array\n",
    "        # :display boolean that toggles whether masks should be shown with color image\n",
    "        # :urPose 4x4 numpy array or SE3 transform that is the pose of the Nth frame when the images were taken\n",
    "        # colorImage,depthImage = RGBD_Image.color,RGBD_Image.depth\n",
    "        # Returns a tuple of (RedPCD,yellowPCD,bluePCD) corresponding to each block class\n",
    "        \n",
    "        # Detects and segments classes using trained yolov8l-seg model \n",
    "        # Inference step, only return instances with confidence > 0.6\n",
    "        pilImage = Image.fromarray(np.array(colorImage))\n",
    "        result = self.model.predict(pilImage,conf=0.6,save=True)[0]\n",
    "        \n",
    "        redMask = self.getSegmentationMask(result,'Red')\n",
    "        yellowMask = self.getSegmentationMask(result,'Yellow')\n",
    "        blueMask = self.getSegmentationMask(result,'Blue') \n",
    "        \n",
    "        '''\n",
    "        if display:\n",
    "            print(\"Color Image\")\n",
    "            plt.imshow(colorImage)\n",
    "            plt.show()\n",
    "            print(\"Red Mask\")\n",
    "            plt.imshow(redMask * 255,cmap = 'gray')\n",
    "            plt.show()\n",
    "            print(\"Yellow Mask\")\n",
    "            plt.imshow(yellowMask * 255,cmap = 'gray')\n",
    "            plt.show()\n",
    "            print(\"Blue Mask\")\n",
    "            plt.imshow(blueMask * 255, cmap = 'gray')\n",
    "            plt.show()\n",
    "        '''\n",
    "        if display:   \n",
    "            fig,ax = plt.subplots(2,1)\n",
    "            print(\"Color Image and Depth Image\")\n",
    "            ax[0].imshow(colorImage)\n",
    "            ax[0].set_title(\"Color Image\")\n",
    "            ax[1].imshow(depthImage)\n",
    "            ax[1].set_title(\"Depth Image\")\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Masks\")\n",
    "            fig,ax = plt.subplots(3,1)\n",
    "            ax[0].imshow(redMask * 255,cmap='gray')\n",
    "            ax[0].set_title(\"Red Mask\")\n",
    "            ax[1].imshow(yellowMask * 255,cmap='gray')\n",
    "            ax[1].set_title(\"Yellow Mask\")\n",
    "            ax[2].imshow(blueMask * 255,cmap='gray')\n",
    "            ax[2].set_title(\"Blue Mask\")\n",
    "            plt.show()\n",
    "        \n",
    "        redDepthImage = np.multiply(depthImage,redMask.astype(int)).astype('float32')\n",
    "        yellowDepthImage = np.multiply(depthImage,yellowMask.astype(int)).astype('float32')\n",
    "        blueDepthImage = np.multiply(depthImage,blueMask.astype(int)).astype('float32')\n",
    "        \n",
    "        # SEGMENT PCD INTO RED,YELLOW,BLUE BLOCKS    \n",
    "        depthScale = self.real.depthScale\n",
    "        \n",
    "        # Create Segmented RGBD Images for Each Color\n",
    "        redRGDB_Image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            o3d.geometry.Image(colorImage),\n",
    "            o3d.geometry.Image(redDepthImage),\n",
    "            convert_rgb_to_intensity=False,\n",
    "            depth_scale=1\n",
    "        )\n",
    "        \n",
    "        yellowRGDB_Image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            o3d.geometry.Image(colorImage),\n",
    "            o3d.geometry.Image(yellowDepthImage),\n",
    "            convert_rgb_to_intensity=False,\n",
    "            depth_scale=1\n",
    "        )\n",
    "    \n",
    "        blueRGBD_Image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            o3d.geometry.Image(colorImage),\n",
    "            o3d.geometry.Image(blueDepthImage),\n",
    "            convert_rgb_to_intensity=False,\n",
    "            depth_scale=1\n",
    "        )\n",
    "        \n",
    "        # Create Point Clouds for Each Class\n",
    "        redPCD = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "            redRGDB_Image,\n",
    "            self.real.pinholeInstrinsics,\n",
    "            project_valid_depth_only=True\n",
    "        )\n",
    "        yellowPCD = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "            yellowRGDB_Image,\n",
    "            self.real.pinholeInstrinsics,\n",
    "            project_valid_depth_only=True\n",
    "        )\n",
    "        bluePCD = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "            blueRGBD_Image,\n",
    "            self.real.pinholeInstrinsics,\n",
    "            project_valid_depth_only=True\n",
    "        )\n",
    "        '''\n",
    "        # Downsample point cloud's based on realsense voxel_size parameter\n",
    "        redPCD = redPCD.voxel_down_sample(voxel_size=self.real.voxelSize)\n",
    "        yellowPCD = yellowPCD.voxel_down_sample(voxel_size=self.real.voxelSize)\n",
    "        bluePCD = bluePCD.voxel_down_sample(voxel_size=self.real.voxelSize)\n",
    "        '''\n",
    "        redPCD.paint_uniform_color([1,0,0])\n",
    "        yellowPCD.paint_uniform_color([1,1,0])\n",
    "        bluePCD.paint_uniform_color([0,0,1])\n",
    "            \n",
    "        # o3d.visualization.draw([redPCD,yellowPCD,bluePCD])\n",
    "        # o3d.visualization.draw_geometries([redPCD,yellowPCD,bluePCD])\n",
    "        redBlock = Block(\"redBlock\",redPCD,urPose)\n",
    "        yellowBlock = Block(\"yellowBlock\",yellowPCD,urPose)\n",
    "        blueBlock = Block(\"blueBlock\",bluePCD,urPose)\n",
    "        return (redBlock,yellowBlock,blueBlock)\n",
    "        # return (redPCD,yellowPCD,bluePCD)\n",
    "\n",
    "    def displayWorld(self,worldPCD,blocks):\n",
    "        coordFrame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.05)\n",
    "        geometry = [coordFrame]\n",
    "        geometry.append(worldPCD)\n",
    "        for block in blocks:\n",
    "            geometry.append(block.blockPCD)\n",
    "            geometry.append(block.blockAABB)\n",
    "            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.0035)\n",
    "            sphere.transform(np.array(sm.SE3.Trans(block.camFrameCoords)))\n",
    "            geometry.extend([sphere])\n",
    "            '''\n",
    "            print(f\"{block.name}\")\n",
    "            deltas = [\"dx\",\"dy\",\"dz\"]\n",
    "            for i in range(0,len(block.robotCoords)):\n",
    "                print(f\"{deltas[i]}: {block.robotCoords[i]}\")\n",
    "\n",
    "            print(f\"{block.name}\\nCam Coordinates: {block.camCoords}\")\n",
    "            '''\n",
    "            # print(f\"Robot Coordinates: {block.robotCoords}\")\n",
    "        o3d.visualization.draw_geometries(geometry)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MotionPlanner():\n",
    "    def __init__(self,blocks,moveRelative = True):\n",
    "        # :moveRelative boolean - True if the gripper moves to block positions in the gripper frame, false if moving to world frame positions (via rtb_model)\n",
    "        # self.robot_model = RTB_Model()\n",
    "        self.blocks = blocks\n",
    "        self.moveRelative = moveRelative\n",
    "    \n",
    "    def displayOpen3D(self):\n",
    "        pass\n",
    "    \n",
    "    def displaySwift(self):\n",
    "        self.robot_model.initSwiftEnv()\n",
    "        for Block in self.blocks:\n",
    "            self.robot_model.addSwiftBox(pos=Block.blockAABB.get_center())\n",
    "    \n",
    "    def runMovement(self):\n",
    "        # self.ur,self.rtb_model,self.blocks\n",
    "        blueBlock = self.blocks[0]\n",
    "        yellowBlock = self.blocks[1]\n",
    "        xB,yB,zB = yellowBlock.gripperFrameCoords # grasp point in gripper frame\n",
    "        print(f\"Block Coordinate:({xB*1000},{yB*1000},{zB*1000})\")\n",
    "        currentPose = self.ur.getPose() #SE3 Object\n",
    "        R = currentPose.R \n",
    "        pX,pY,pZ = tuple(currentPose.t)\n",
    "        print(f\"Current Pose:\\n{currentPose*1000}\")\n",
    "        print(f\"Pose Coordinate: ({pX*1000},{pY*1000},{pZ*1000})\")\n",
    "        if self.moveRelative == False:\n",
    "            # Move directly to block position in world frame\n",
    "            goalPose = copy.deepcopy(currentPose)\n",
    "            goalPose.t[0] = xB\n",
    "            goalPose.t[1] = yB\n",
    "        else:\n",
    "            # Move relative to current position given block position in gripper frame\n",
    "            # subtract 0.175 from block position to account for gripper length\n",
    "            zB -= 0.165\n",
    "            pX,pY,pZ = np.array(currentPose.t) \n",
    "            print(f\"pZ:{pZ}\")\n",
    "            # xB,yB,zB here is the block position in the gripper frame which is aligned with the optoforce frame\n",
    "            R = self.ur.getPose().R\n",
    "            P_goal = np.matmul(R,np.array([xB,yB,zB]).T)  # relative position of the block in world coordinates\n",
    "            goalX,goalY,goalZ = tuple(P_goal) # quantities and directions the the gripper frame should be incremented to be centered at the block \n",
    "            goalPose = copy.deepcopy(currentPose) # maintain rotation and shift position\n",
    "            print(f\"P_goal:\\n{P_goal}\")\n",
    "            goalPose.t[0] += goalX\n",
    "            goalPose.t[1] += goalY\n",
    "           \n",
    "        \n",
    "        print(f\"Goal Coordinate ({goalPose.t[0]*1000},{goalPose.t[1]*1000},{goalPose.t[2]*1000})\")\n",
    "        print(\"Moving to goal\")\n",
    "        print(f\"Goal Pose\\n {goalPose}\")\n",
    "        # self.ur.moveL(goalPose)\n",
    "        goalPose.t[2] += goalZ\n",
    "        # self.ur.moveL(goalPose)\n",
    "        # self.ur.closeGripper(18)\n",
    "\n",
    "'''\n",
    "detector = ObjectDetection()\n",
    "\n",
    "try:\n",
    "    pcd,rgbdImage = detector.real.getPCD()\n",
    "    # rgbdImage = detector.real.takeImages()\n",
    "    depthImage,colorImage = rgbdImage.depth,rgbdImage.color\n",
    "    detector.real.displayImages(depthImage,colorImage)\n",
    "    redPCD,yellowPCD,bluePCD = detector.colorSegmentation(colorImage,depthImage)\n",
    "    # detector.real.displayPCD([pcd])\n",
    "    detector.real.displayPCD([redPCD,yellowPCD,bluePCD])\n",
    "    blocks = [Block(\"Red Block\",redPCD),Block(\"Yellow Block\",yellowPCD),Block(\"Blue Block\",bluePCD)]\n",
    "    items = []\n",
    "    for block in blocks:\n",
    "        s = o3d.geometry.TriangleMesh.create_sphere(radius=0.0125/4)\n",
    "        s.translate(block.getGraspPoint())\n",
    "        items.extend([block.blockPCD,block.blockAABB,s])\n",
    "    detector.real.displayPCD(items)\n",
    "    m = MotionPlanner(blocks)\n",
    "    m.display()\n",
    "    \n",
    "    # o3d.io.write_point_cloud(\"redPCD.pcd\",redPCD)\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    detector.real.pipe.stop()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TASK PLANNER CODE\n",
    "from py2pddl import Domain, create_type, predicate,action,init,goal\n",
    "class blocksDomain(Domain):\n",
    "    Object = create_type(\"Object\")\n",
    "    \n",
    "    @predicate(Object)\n",
    "    def Block(self,objectA):\n",
    "         # true if objectA is a block\n",
    "        pass\n",
    "    \n",
    "    @predicate(Object)\n",
    "    def fixed(self,objectA):\n",
    "         # true if objectA is fixed\n",
    "        pass\n",
    "\n",
    "    @predicate(Object,Object)\n",
    "    def on(self,objectA,objectB):\n",
    "         # true if objectA is on objectB\n",
    "        pass\n",
    "\n",
    "    @predicate(Object)\n",
    "    def clear(self,objectA):\n",
    "        # true if blockA can be grasped without knocking over other blocks i.e. blockA is on top\n",
    "        pass\n",
    "    \n",
    "    @action(Object,Object,Object)\n",
    "    def move(self,block,underObject,newUnderObject):\n",
    "        # precondition is that block is of type Block\n",
    "        # underObject is object currently underneath block (Location or Block)\n",
    "        # newUnderObject is object desired to be underneath block \n",
    "        precond = [~self.fixed(block),self.Block(block),self.on(block,underObject),self.clear(block),self.clear(newUnderObject)]\n",
    "        effect = [~self.on(block,underObject),self.on(block,newUnderObject),self.clear(block),self.clear(underObject),~self.clear(newUnderObject)]\n",
    "        return precond,effect\n",
    "\n",
    "class blocksProblem(blocksDomain):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.objects = blocksDomain.Object.create_objs([\"redBlock\",\"yellowBlock\",\"blueBlock\",\"loc-a\",\"loc-b\",\"loc-c\"],prefix=\"\")\n",
    "    \n",
    "    @init\n",
    "    def init(self,initDict) -> list:\n",
    "        # initDicts keys are <predicateName>\n",
    "        # initDict values are objects the predicate holds for as string\n",
    "        initState = []\n",
    "        for predicateName in initDict:\n",
    "            # Objects the predicate holds for (can be a single string or a list)\n",
    "            for objects in initDict[predicateName]:\n",
    "                if predicateName == \"on\":\n",
    "                    newPredicate = self.on(self.objects[objects[0]],self.objects[objects[1]])\n",
    "                    \n",
    "                elif predicateName == \"fixed\":\n",
    "                    newPredicate = self.fixed(self.objects[objects])\n",
    "                    \n",
    "                elif predicateName == \"clear\":\n",
    "                    newPredicate = self.clear(self.objects[objects])\n",
    "                    \n",
    "                elif predicateName == \"Block\":\n",
    "                    newPredicate = self.Block(self.objects[objects])\n",
    "                initState.append(newPredicate)\n",
    "        return initState\n",
    "\n",
    "    @goal\n",
    "    def goal(self,goalDict) -> list:\n",
    "        # initDicts keys are <predicateName>\n",
    "        # initDict values are objects the predicate holds for as string\n",
    "        # initDict values are objects the predicate holds for as string\n",
    "        goalState = []\n",
    "        for predicateName in goalDict:\n",
    "            # Objects the predicate holds for (can be a single string or a list)\n",
    "            for objects in goalDict[predicateName]:\n",
    "                if predicateName == \"on\":\n",
    "                    newPredicate = self.on(self.objects[objects[0]],self.objects[objects[1]])\n",
    "                    \n",
    "                elif predicateName == \"fixed\":\n",
    "                    newPredicate = self.fixed(self.objects[objects])\n",
    "                    \n",
    "                elif predicateName == \"clear\":\n",
    "                    newPredicate = self.clear(self.objects[objects])\n",
    "                    \n",
    "                elif predicateName == \"Block\":\n",
    "                    newPredicate = self.Block(self.objects[objects])\n",
    "                goalState.append(newPredicate)\n",
    "        return goalState\n",
    "        \n",
    "        '''\n",
    "        goalTruths = [\n",
    "            self.on(self.objects[\"blueBlock\"],self.objects[\"loc-b\"]),\n",
    "            self.on(self.objects[\"yellowBlock\"],self.objects[\"blueBlock\"]),\n",
    "            self.on(self.objects[\"redBlock\"],self.objects[\"loc-c\"])\n",
    "        ]\n",
    "        '''\n",
    "        '''\n",
    "        goalTruths = [\n",
    "            self.on(self.objects[\"blueBlock\"],self.objects[\"redBlock\"]),\n",
    "            self.on(self.objects[\"redBlock\"],self.objects[\"loc-a\"]),\n",
    "            self.on(self.objects[\"yellowBlock\"],self.objects[\"redBlock\"])\n",
    "        ]\n",
    "        '''\n",
    "        '''\n",
    "        goalTruths = [\n",
    "            self.on(self.objects[\"blueBlock\"],self.objects[\"loc-b\"])\n",
    "        ]\n",
    "        '''\n",
    "        '''\n",
    "        goalTruths = [\n",
    "            self.on(self.objects[\"blueBlock\"],self.objects[\"redBlock\"])\n",
    "        ]\n",
    "        '''\n",
    "        '''\n",
    "        goalTruths = [\n",
    "            self.on(self.objects[\"redBlock\"],self.objects[\"loc-c\"]),\n",
    "            self.on(self.objects[\"yellowBlock\"],self.objects[\"redBlock\"]),\n",
    "            self.on(self.objects[\"blueBlock\"],self.objects[\"yellowBlock\"])\n",
    "        ]\n",
    "        '''\n",
    "        return goalTruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task Planner Code Continues\n",
    "class TaskPlanner():\n",
    "    def __init__(self,blocks):\n",
    "        self.blocks = blocks\n",
    "        self.blockConnections = None\n",
    "        self.locPositions = {} # dictionary that maps from location names to gripper frame coordinates\n",
    "        pass\n",
    "\n",
    "    def blockOn(self,blockA,blockB):\n",
    "        # returns true if blockA is on blockB based on rough world coordinates\n",
    "        # returning false implies that block is on the table\n",
    "        # CONDITIONALS MAY NEED TO CHANGE IF ACTUAL UR5 COORDINATE SYSTEM DOESN'T MATCH DRAWING (FOR INSTANCE CHECK BOUNDS IN X RATHER THAN Y)\n",
    "        aX,aY,aZ = blockA.worldFrameCoords\n",
    "        bX,bY,bZ = blockB.worldFrameCoords\n",
    "        # CHANGE BLOCK LENGTH FOR REAL WORLD EXPERIMENTS WITH DIFFERENT SIZED BLOCK (0.1)\n",
    "        blockLength = 0.02 # meters (20 mm)\n",
    "        # Check if the distance between the centers is within 60% of the blockLength in the x-axis \n",
    "        if abs(bX-aX) <= (0.5*blockLength*1.2):        \n",
    "            # check if z-axis distance between the centers is within 150% of the blockLength with block a being on top of b\n",
    "            if abs(bZ-aZ) <= (1.5 * blockLength) and aZ > bZ:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def getProblemArguments(self,blocks):\n",
    "        # returns tuple of (initDict,locPositions) \n",
    "        # initDict contains values passed into init\n",
    "        # locPositions is a dict that maps from loc name to gripperFrameCoordinates\n",
    "        blockPairs = set()\n",
    "        for blockA in blocks:\n",
    "            for blockB in blocks:\n",
    "                if blockA!=blockB:\n",
    "                    blockPairs.add((blockA,blockB))\n",
    "\n",
    "\n",
    "        blockConnectionsBelow = {} # graph as dict mapping from block to block below it\n",
    "        for blockA,blockB in blockPairs:\n",
    "            if self.blockOn(blockA,blockB):\n",
    "                blockConnectionsBelow[blockA.name] = blockB.name\n",
    "\n",
    "        # print(blockConnectionsBelow)\n",
    "        # This would randomly assign loc to be below bottom level blocks but this will be hard-coded\n",
    "        # Order from left to right based on world coordinate X (loc-a is at xMax)\n",
    "        tablePos = [\"loc-a\",\"loc-b\",\"loc-c\"]\n",
    "        bottomBlocks = [] # list containing tuples of lowest left blocks and their x position\n",
    "        for block in blocks:\n",
    "            if block.name not in blockConnectionsBelow:\n",
    "                # if a block has nothing below it\n",
    "                bottomBlocks.append((block.worldFrameCoords[0],block))\n",
    "\n",
    "        # Order bottom level blocks by decreasing x value\n",
    "        bottomBlocks.sort()\n",
    "        bottomBlocks.reverse()\n",
    "\n",
    "        \n",
    "        # loc's will have same positions as blocks, but they should really be below them which requires projection\n",
    "        locPositions = {} # maps from loc name to position in gripper frame\n",
    "\n",
    "        # Method assumes there is enough room between the left and rightmost blocks for loc b (cannot have a stack only in locA,locB with locC on right and empty)\n",
    "        if len(bottomBlocks) >= 2:\n",
    "            # If there are 2-3 blocks at the bottom assign loc-a to leftmost, loc-c to rightmost\n",
    "            leftBlock = bottomBlocks[0][1]\n",
    "            rightBlock = bottomBlocks[-1][1]\n",
    "            blockConnectionsBelow[leftBlock.name] = \"loc-a\"\n",
    "            blockConnectionsBelow[rightBlock.name] = \"loc-c\"\n",
    "            self.locPositions[\"loc-a\"] = leftBlock.gripperFrameCoords\n",
    "            self.locPositions[\"loc-c\"] = rightBlock.gripperFrameCoords\n",
    "\n",
    "            if len(bottomBlocks) == 3:\n",
    "                # If there are 3 blocks at bottom also assign loc-b to middle blocks\n",
    "                middleBlock = bottomBlocks[1][1]\n",
    "                blockConnectionsBelow[middleBlock.name] = \"loc-b\"\n",
    "                self.locPositions[\"loc-b\"] = middleBlock.gripperFrameCoords \n",
    "            else:\n",
    "                # If 2 bottom blocks only then loc-b is between left and right block\n",
    "                l = np.array([leftBlock.gripperFrameCoords,rightBlock.gripperFrameCoords])\n",
    "                x,y,z = np.mean(l[:,0]),np.mean(l[:,1]),np.mean(l[:,2])   \n",
    "                self.locPositions[\"loc-b\"] = np.array([x,y,z])\n",
    "\n",
    "        blockConnectionsBelow[\"loc-a\"] = None\n",
    "        blockConnectionsBelow[\"loc-b\"] = None\n",
    "        blockConnectionsBelow[\"loc-c\"] = None\n",
    "\n",
    "        # print(blockConnectionsBelow)\n",
    "\n",
    "        # problemClass = blocksProblem()\n",
    "        # problemClass.objects = blocksDomain.Object.create_objs([\"redBlock\",\"yellowBlock\",\"blueBlock\",\"loc-a\",\"loc-b\",\"loc-c\"],prefix=\"\")\n",
    "\n",
    "\n",
    "        initDict = {} # Initially true predicates\n",
    "        # Keys in initDict correspond to parameter names in the blockDomain.init() function \n",
    "        # with the value stored under the key passed into init() with the name of the key\n",
    "        # assert that some objects are of type block\n",
    "        # Here, keys are predicateNames and values are lists of of objects they hold for\n",
    "        # This is used for interfacing with Py2PDDL and initializing the state in the problemClass\n",
    "\n",
    "        predicateNames = ['Block','on','fixed','clear']\n",
    "        # Each predicate initially acts on no objects []\n",
    "        for predicateName in predicateNames:\n",
    "            initDict[predicateName] = []\n",
    "\n",
    "        initDict['Block'] = ['redBlock','blueBlock','yellowBlock']    \n",
    "        print(\"<<INITIAL WORLD STATE>>\")\n",
    "        for Object in blockConnectionsBelow:\n",
    "            objectBelow = blockConnectionsBelow[Object]\n",
    "            if objectBelow != None:\n",
    "                print(f\"on({Object},{objectBelow})\")\n",
    "                initDict[\"on\"].append((Object,objectBelow))\n",
    "\n",
    "            else:\n",
    "                # bottom level object has objectBelow=None and is location therefore has 'fixed' property\n",
    "                print(f\"fixed({Object})\")\n",
    "                initDict[\"fixed\"].append(Object)\n",
    "\n",
    "            # check if block is the top layer block i.e not the child (below) any other blocks\n",
    "            if not Object in blockConnectionsBelow.values():\n",
    "                print(f\"clear({Object})\")\n",
    "                initDict['clear'].append(Object)\n",
    "\n",
    "        # print(f\"initDict:\\n{initDict}\")\n",
    "        return initDict\n",
    "\n",
    "\n",
    "    def generatePDDLFiles(self,blocks):\n",
    "        # generates domain.pddl and problem.pddl files based on the problem object and superclass which specifies domain\n",
    "        initDict = self.getProblemArguments(blocks)\n",
    "        problem = blocksProblem()\n",
    "        problem.generate_domain_pddl()\n",
    "        problem.generate_problem_pddl(\n",
    "            init = {\n",
    "                'initDict':initDict\n",
    "            },\n",
    "            goal = {\n",
    "                'goalDict':self.goalDict\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def generatePDDLPlan(self):\n",
    "        # precondition: call generatePDDLFiles prior to calling this\n",
    "        # generates plan based on domain.pddl and problem.pddl using fast downward which is writtent to sas_plan file\n",
    "        print(\"<<RUNNING FASTDOWNWARD PLANNER>>\")\n",
    "        os.system(\"./downward/fast-downward.py domain.pddl problem.pddl --search 'astar(lmcut())'\")\n",
    "    \n",
    "    def parsePlan(self,blocks):\n",
    "        # precondition: call generatePDDLPlan prior to calling this\n",
    "        # parses outputted plan in the sas_plan file into a sequence of coordinates to visit\n",
    "        planFile = open(\"./sas_plan\",\"r\")\n",
    "        fileText = planFile.read()\n",
    "        planFile.close()\n",
    "        steps = []\n",
    "        # coordTranslator = getTranslator(blocks)\n",
    "\n",
    "        # dictionary with keys equal to object names and values equal to Object class\n",
    "        # only blocks for now, would be good to expand to locations as well\n",
    "        nameMap = {}\n",
    "        for block in self.blocks:\n",
    "            nameMap[block.name] = block\n",
    "        \n",
    "        print(\"<<PLANNER OUTPUT>>\")\n",
    "        for actionLine in fileText.split(\"\\n\")[0:-2]:\n",
    "            actionLine = actionLine.replace(\"(\",\"\")\n",
    "            actionLine = actionLine.replace(\")\",\"\")\n",
    "            # only can parse move commands for now as number of params varies for other actions\n",
    "            # compensating with fast downward converting to lowercase\n",
    "            # can fix with refactor\n",
    "            actionLine = actionLine.replace(\"block\",\"Block\")\n",
    "            #print(actionLine.split(\" \"))\n",
    "            actionName, objectToMove,whereToMoveFrom,whereToMoveTo = actionLine.split(\" \")\n",
    "            # print(f\"Action: {actionName}\\nobjectToMove: {objectToMove}\\nUnder Object: {whereToMoveFrom}\\nTo: {whereToMoveTo}\")\n",
    "            print(f\"Action: {actionName}\\nobjectToMove: {objectToMove}\\nTo: {whereToMoveTo}\")\n",
    "            blockToMove = copy.deepcopy(nameMap[objectToMove]) # The block that needs to be moved\n",
    "            pickupCoords = copy.deepcopy(blockToMove.gripperFrameCoords)\n",
    "            print(f\"Pickup {blockToMove.name} at {pickupCoords}\")\n",
    "            # if whereToMoveTo is a block vs. a location handle it different\n",
    "            # if a location then don't add the block height\n",
    "            if whereToMoveTo in [\"loc-a\",\"loc-b\",\"loc-c\"]:\n",
    "                # This extra distance shouldn't be needed but we'll add it anyways\n",
    "                releaseCoords = copy.deepcopy(self.locPositions[whereToMoveTo])\n",
    "                print(f\"Release on object {whereToMoveTo} at position {releaseCoords}\")\n",
    "            else:\n",
    "                goalBlock = copy.deepcopy(nameMap[whereToMoveTo])\n",
    "                # move to a block then add block length to position loc's are already adjusted)  \n",
    "                goalPostion = copy.deepcopy(nameMap[whereToMoveTo])\n",
    "                blockLength = 0.02\n",
    "                releaseCoords = goalBlock.gripperFrameCoords + goalBlock.getWorldFrameVerticalInGripper(blockLength)\n",
    "                print(f\"Release on object {whereToMoveTo} at position {releaseCoords}\")\n",
    "            # After moving a block we need to update its position\n",
    "            blockToMove.gripperFrameCoords = copy.deepcopy(releaseCoords)\n",
    "            \n",
    "            # Steps contains a sequence of pickup and release coordinates\n",
    "            steps.append((pickupCoords,releaseCoords))\n",
    "            '''\n",
    "            pickupCoords = coordTranslator[objectToMove]\n",
    "            releaseCoords = coordTranslator[whereToMoveTo]\n",
    "            # drop just above block\n",
    "            blockLength = 0.02\n",
    "            releaseCoords[2] -= blockLength\n",
    "            print(f\"Pick up {objectToMove} at {pickupCoords}, Release at {releaseCoords}\\n\")\n",
    "            steps.append((pickupCoords,releaseCoords))\n",
    "            # update object location reference\n",
    "            coordTranslator[objectToMove] = releaseCoords\n",
    "            '''\n",
    "        return steps\n",
    "    \n",
    "    def generatePlan(self,goalDict):\n",
    "        # Returns a sequence of positions that blocks should be grasped and then released relative to the current gripper frame\n",
    "        '''\n",
    "        for block in self.blocks:\n",
    "            print(f\"{block.name}: {block.worldFrameCoords * 1000}\")\n",
    "        '''\n",
    "        self.goalDict = goalDict\n",
    "        self.generatePDDLFiles(self.blocks)\n",
    "        self.generatePDDLPlan()\n",
    "        steps = self.parsePlan(self.blocks)\n",
    "        return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (0) - Hardware Discovery\n",
    "# Call this once to intialize serial connections to ur and gripper\n",
    "\n",
    "# robotIP = \"192.168.0.6\"\n",
    "# con = rtde_control.RTDEControlInterface(robotIP)\n",
    "# rec = rtde_receive.RTDEReceiveInterface(robotIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (0) - Gripper Discovery\n",
    "# To list serial ports of the motor interface\n",
    "# $ python -m serial.tools.list_ports\n",
    "servoPort = \"/dev/ttyACM0\"\n",
    "gripperController = Motors(servoPort)\n",
    "gripperController.torquelimit(600) # used to be 600\n",
    "gripperController.speedlimit(100)\n",
    "ur = UR5_Interface()\n",
    "ur.gripperController = gripperController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (1) - Hardware Interface Initialization\n",
    "try:\n",
    "    robotIP = \"192.168.0.6\"\n",
    "    con = rtde_control.RTDEControlInterface(robotIP)\n",
    "    rec = rtde_receive.RTDEReceiveInterface(robotIP)\n",
    "    ur = UR5_Interface()\n",
    "    ur.c = con\n",
    "    ur.r = rec\n",
    "    ur.gripperController = gripperController\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "else:\n",
    "    print(\"UR5 + Gripper Interface Established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.c.disconnect()\n",
    "ur.r.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (2) - Hardware Test - Raises gripper 1 cm and open's closes gripper\n",
    "ur.testRoutine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (3) - Initialize connection to RealSense\n",
    "real = RealSense()\n",
    "real.initConnection()\n",
    "# real.displayStream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (4) - RealSense and YOLO Initialization\n",
    "# robot_model steps can deleted once extrinsic-free transforms are verfied to be correct\n",
    "# robot_model = RTB_Model()\n",
    "# robot_model.setJointAngles(ur.getJointAngles())\n",
    "try:\n",
    "    detector = ObjectDetection(real,None,moveRelative = True)\n",
    "except Exception as e:\n",
    "    detector.real.pipe.stop()\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.openGripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (5) - Image Measurements, Segmentation, and Processing into Blocks\n",
    "urPose = ur.getPose()\n",
    "pcd,rgbdImage = detector.real.getPCD()\n",
    "depthImage,colorImage = rgbdImage.depth,rgbdImage.color\n",
    "blocks = detector.getBlocksFromImages(colorImage,depthImage,urPose,display = True)\n",
    "for block in blocks:\n",
    "    print(f\"{block.name}:\")\n",
    "    print(f\"CamFrameCoords: {block.camFrameCoords}\")\n",
    "    print(f\"GripperFrameCoords: {block.gripperFrameCoords}\")\n",
    "    print(f\"WorldFrameCoords: {block.worldFrameCoords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (4) - Displaying PCD\n",
    "detector.displayWorld(pcd,blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.openGripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (5) - Task Planning\n",
    "planner = TaskPlanner(blocks)\n",
    "goalDict = {\"on\":[(\"blueBlock\",\"yellowBlock\")]}\n",
    "steps = planner.generatePlan(goalDict)\n",
    "print(steps)\n",
    "for block in blocks:\n",
    "    print(f\"{block.name} - {list(block.gripperFrameCoords)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (6) - Grasping blocks at each position in steps, returning to start position, moving to release position, moving back to start, opening gripper\n",
    "sleepRate = 0.75\n",
    "def projectToWorldCoords(gripperFrameCoords):\n",
    "    # given a goal position in gripper coords returns the displacements from the current pose in world coords\n",
    "    xB,yB,zB = gripperFrameCoords\n",
    "    # subtract 0.165 from block position in gripper frame to account for gripper length\n",
    "    zB -= 0.155\n",
    "    currentPose = ur.getPose() #SE3 Object\n",
    "    # print(f\"Current Pose:\\n{currentPose*1000}\")\n",
    "    R = currentPose.R \n",
    "    pX,pY,pZ = tuple(currentPose.t)\n",
    "    # xB,yB,zB here is the block position in the gripper frame which is aligned with the optoforce frame\n",
    "    P_goal = np.matmul(R,np.array([xB,yB,zB]).T)  # relative position of the block in world coordinates\n",
    "    print(f\"P_goal:\\n{P_goal}\")\n",
    "    dX,dY,dZ = tuple(P_goal) # quantities and directions the the gripper frame should be incremented to be centered at the block \n",
    "    return dX,dY,dZ\n",
    "    \n",
    "def moveToBlock(blockPos):\n",
    "    # would be better if this was block object\n",
    "    # :blockPos is coord in gripper frame\n",
    "    dX,dY,dZ = projectToWorldCoords(blockPos) # goalPose in world coordinates\n",
    "    homePose = ur.getPose()\n",
    "    dZ  += 7/1000 # up 7 mm to avoid hitting lower block\n",
    "    goal1 = copy.deepcopy(homePose)\n",
    "    goal1.t[2] += dZ\n",
    "    ur.moveL(goal1)\n",
    "    time.sleep(sleepRate)\n",
    "    goal2 = goal1\n",
    "    goal2.t[0] += dX\n",
    "    goal2.t[1] += dY\n",
    "    ur.moveL(goal2)\n",
    "    time.sleep(sleepRate)\n",
    "    \n",
    "def moveBackFromBlock(homePose):    \n",
    "    currentPose = ur.getPose()\n",
    "    # Move up 3 mm to avoid raise block to prevent friction from toppling lower block\n",
    "    goal1 = copy.deepcopy(currentPose)\n",
    "    goal1.t[2] += 3/1000\n",
    "    ur.moveL(goal1)\n",
    "    time.sleep(sleepRate)\n",
    "    currentPose = ur.getPose()\n",
    "    dX,dY,dZ = tuple(homePose.t - currentPose.t)\n",
    "    # Move in the XY Plane then Z Axis\n",
    "    goal2 = copy.deepcopy(currentPose)\n",
    "    goal2.t[0] += dX\n",
    "    goal2.t[1] += dY\n",
    "    ur.moveL(goal2)\n",
    "    time.sleep(sleepRate)\n",
    "    # Move in Z Axis back to home\n",
    "    goal3 = copy.deepcopy(goal2)\n",
    "    goal3.t[2] += dZ\n",
    "    ur.moveL(goal3)\n",
    "    time.sleep(sleepRate)\n",
    "\n",
    "\n",
    "    \n",
    "goalBlock = blocks[1]\n",
    "blockLength = 0.02\n",
    "releaseCoords = goalBlock.gripperFrameCoords + goalBlock.getWorldFrameVerticalInGripper(blockLength)\n",
    "verticalDist = 0.02\n",
    "gX,gY,gZ = tuple(goalBlock.urPose.t)\n",
    "res = np.matmul(goalBlock.urPose.R,(sm.SE3.Trans([gX,gY,gZ+verticalDist]).t - goalBlock.urPose.t))\n",
    "# print(f\"res: {projectToWorldCoords(res)} \")\n",
    "# ur.openGripper() # Open gripper\n",
    "# ur.testRoutine()\n",
    "# homePose = ur.getPose()\n",
    "'''\n",
    "for step in steps:\n",
    "    # Grasp and Move Home Step\n",
    "    grabPos,releasePos = step\n",
    "    moveToBlock(grabPos) \n",
    "    print(\"Done moving to block\")\n",
    "    ur.closeGripper(9) \n",
    "    time.sleep(sleepRate)\n",
    "    moveBackFromBlock(homePose)\n",
    "    moveToBlock(releasePos)\n",
    "    ur.closeGripper(55)\n",
    "    moveBackFromBlock(homePose)\n",
    "    ur.openGripper()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.c.disconnect()\n",
    "ur.r.disconnect()\n",
    "time.sleep(2)\n",
    "try:\n",
    "    robotIP = \"192.168.0.6\"\n",
    "    con = rtde_control.RTDEControlInterface(robotIP)\n",
    "    rec = rtde_receive.RTDEReceiveInterface(robotIP)\n",
    "    ur = UR5_Interface()\n",
    "    ur.c = con\n",
    "    ur.r = rec\n",
    "    ur.gripperController = gripperController\n",
    "    time.sleep(5)\n",
    "    ur.testRoutine()\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "else:\n",
    "    print(\"UR5 + Gripper Interface Established\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "planner.locPositions[\"loc-a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.moveL(homePose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.openGripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ur.moveL(homePose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.openGripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAY WANT TO BREAK THIS UP INTO SEPARATE CELLS\n",
    "def runRoutine():\n",
    "    print(\"1\")\n",
    "    try:\n",
    "        ur = UR5_Interface()\n",
    "        ur.c = con\n",
    "        ur.r = rec\n",
    "        ur.gripperController = gripperController\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "        \n",
    "    print(\"2\")\n",
    "    # robot_model = RTB_Model()\n",
    "    # robot_model.setJointAngles(ur.getJointAngles())\n",
    "    # Set joint angles so object detection has correct extrinsics\n",
    "    \n",
    "    # Instantiates ObjectDetection object which intializes a connection to the realsense\n",
    "    detector = ObjectDetection(robot_model,moveRelative = True)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        ur.openGripper()\n",
    "        print(f\"Nth Frame Pose:\\n{np.array(ur.getPose())}\")\n",
    "        # Takes images for display\n",
    "        pcd,rgbdImage = detector.real.getPCD()\n",
    "        depthImage,colorImage = rgbdImage.depth,rgbdImage.color\n",
    "        # detector.real.displayImages(depthImage,colorImage)\n",
    "        \n",
    "        blocks = detector.getBlocksFromImages(colorImage,depthImage)\n",
    "        for block in blocks:\n",
    "            print(f\"{block.name}:\")\n",
    "            print(f\"CamFrameCoords: {block.camFrameCoords}\")\n",
    "            print(f\"GripperFrameCoords: {block.gripperFrameCoords}\")\n",
    "            print(f\"WorldFrameCoords: {block.worldFrameCoords}\")\n",
    "        m = MotionPlanner(blocks,moveRelative = True)\n",
    "        # Interface to the UR5\n",
    "        m.ur = ur \n",
    "        # m.rtb_model = robot_model\n",
    "        redBlock,yellowBlock,blueBlock = blocks\n",
    "        redPCD,yellowPCD,bluePCD = redBlock.blockPCD,yellowBlock.blockPCD,blueBlock.blockPCD\n",
    "        redAABB,yellowAABB,blueAABB = redBlock.blockAABB,yellowBlock.blockAABB,blueBlock.blockAABB,\n",
    "        detector.real.displayPCD([redPCD,yellowPCD,bluePCD,redAABB,yellowAABB,blueAABB])\n",
    "        m.runMovement()\n",
    "        # time.sleep(3)\n",
    "        # ur.closeGripper(10)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "    finally:\n",
    "        detector.real.pipe.stop()\n",
    "        # ur.c.disconnect()\n",
    "        # ur.r.disconnect()\n",
    "runRoutine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.c.disconnect()\n",
    "ur.r.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "robot_model = RTB_Model()\n",
    "robot_model.plotRobot()\n",
    "x,y,z = [],[],[]\n",
    "for point in redPCD.points:\n",
    "    x.append(point[0])\n",
    "    y.append(point[1])\n",
    "    z.append(point[2])\n",
    "\n",
    "print(np.mean(x))\n",
    "print(np.mean(y))\n",
    "print(np.mean(z))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(x,y,z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(real.extrinsics is None) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
