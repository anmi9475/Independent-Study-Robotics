{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dylan Kriegman / May 2023\n",
    "# modified by Andrea Miller / Fall 2023\n",
    "#modified by Jensen Lavering / Fall 2023\n",
    "# usb port for dynamixel: tty.usbmodem14101\n",
    "\n",
    "# idea: failutre recovery << screws example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Succeeded to open the port\n",
      "Succeeded to change the baudrate\n",
      "Moving speed of dxl ID: 1 set to 100 \n",
      "Moving speed of dxl ID: 2 set to 100 \n",
      "we made it here\n",
      "Moving speed of dxl ID: 1 set to 100 \n",
      "Moving speed of dxl ID: 2 set to 100 \n",
      "UR5 + Gripper Interface Established\n",
      "Detector established\n",
      "Joint Angles:  [     47.245     -94.901      147.33     -41.989      47.762     -8.6615]\n"
     ]
    }
   ],
   "source": [
    "%run GetRobotPose.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Succeeded to open the port\n",
      "Succeeded to change the baudrate\n",
      "Moving speed of dxl ID: 1 set to 100 \n",
      "Moving speed of dxl ID: 2 set to 100 \n",
      "Moving speed of dxl ID: 1 set to 100 \n",
      "Moving speed of dxl ID: 2 set to 100 \n",
      "UR5 + Gripper Interface Established\n",
      "Detector established\n",
      "Joint Angles:  [     23.117     -80.663      126.07       11.03      23.082     -56.382]\n",
      "pos:  [  -0.055914    -0.53808    0.029902      1.8719    -0.36717    -0.28374]\n",
      "pos:  [   -0.13942    -0.54297    0.056392      1.8972     0.20627    0.094679]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Blue, 1 Red, 1 Yellow, 51.1ms\n",
      "Speed: 2.4ms preprocess, 51.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1m/home/andrea/Independent-Study-Robotics/runs/segment/predict246\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully closed port\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Independent-Study-Robotics/Full-System/DisplayPCD.py:119\u001b[0m\n\u001b[1;32m    117\u001b[0m ur\u001b[38;5;241m.\u001b[39mr\u001b[38;5;241m.\u001b[39mdisconnect()\n\u001b[1;32m    118\u001b[0m real\u001b[38;5;241m.\u001b[39mdisconnect()\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m (e)\n",
      "File \u001b[0;32m~/Independent-Study-Robotics/Full-System/DisplayPCD.py:95\u001b[0m\n\u001b[1;32m     90\u001b[0m pcd, rgbds \u001b[38;5;241m=\u001b[39m get_pcd_at_multiple_positions(ur, real)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# switch get_pcd_at_multiple_positions method to return list of pcds\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# use displayPCD from RealSense class\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m blocks \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetBlocksFromImages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgbds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# need a method to get the best set of blocks from blocksList\u001b[39;00m\n\u001b[1;32m    102\u001b[0m planner \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mTaskPlanner(blocks)\n",
      "File \u001b[0;32m~/Independent-Study-Robotics/Full-System/ObjectDetection.py:165\u001b[0m, in \u001b[0;36mObjectDetection.getBlocksFromImages\u001b[0;34m(self, images, display)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetBlocksFromImages\u001b[39m(\u001b[38;5;28mself\u001b[39m, images: List[TorchImage], display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# :colorImage 3-channel rgb image as numpy array\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# :depthImage 1-channel of measurements in z-axis as numpy array\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# Detects and segments classes using trained yolov8l-seg model\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Inference step, only return instances with confidence > 0.6\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     image_packets: List[ImagePacket] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pcds_of_all_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# avg world frame coords for all blocks\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     redBlock \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Independent-Study-Robotics/Full-System/ObjectDetection.py:103\u001b[0m, in \u001b[0;36mObjectDetection.get_pcds_of_all_images\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    101\u001b[0m pilImage \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39marray(image\u001b[38;5;241m.\u001b[39mcolorImage))\n\u001b[1;32m    102\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(pilImage, conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 103\u001b[0m redMask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetSegmentationMask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m yellowMask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetSegmentationMask(result, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYellow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m blueMask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetSegmentationMask(result, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Independent-Study-Robotics/Full-System/ObjectDetection.py:84\u001b[0m, in \u001b[0;36mObjectDetection.getSegmentationMask\u001b[0;34m(self, result, className)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetSegmentationMask\u001b[39m(\u001b[38;5;28mself\u001b[39m, result, className):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# :result ultralytics.result\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# :className string corresponding to label in trained YOLO model\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Here, className should be in {'Red','Yellow','Blue'}\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# Returns 1st instance of the class as binary numpy array and None if the class is not present\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     classList \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(result\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mcls))\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(classList)):\n\u001b[1;32m     86\u001b[0m         predictedClassName \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mnames[classList[i]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cpu' is not defined"
     ]
    }
   ],
   "source": [
    "#This runs and pops up the PCD and other related data then disconnects everything! No need to restart Kernal!!!!!\n",
    "%run DisplayPCD.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planFile = open(\"./sas_plan\", \"r\")\n",
    "fileText = planFile.read()\n",
    "fileText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run RunBlockTest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start experiment here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (0) - Hardware Discovery\n",
    "# Call this once to intialize serial connections to ur and gripper\n",
    "import rtde_control\n",
    "import rtde_receive\n",
    "robotIP = \"192.168.0.6\"\n",
    "con = rtde_control.RTDEControlInterface(robotIP)\n",
    "rec = rtde_receive.RTDEReceiveInterface(robotIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (0) - Gripper Discovery\n",
    "# To list serial ports of the motor interface\n",
    "# $ python -m serial.tools.list_ports\n",
    "from Motor_Code import Motors\n",
    "import UR5_Interface as ur\n",
    "servoPort = \"/dev/ttyACM0\"\n",
    "gripperController = Motors(servoPort)\n",
    "gripperController.torquelimit(600) # used to be 600\n",
    "gripperController.speedlimit(100)\n",
    "ur = ur.UR5_Interface()\n",
    "ur.gripperController = gripperController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (1) - Hardware Interface Initialization\n",
    "try:\n",
    "    ur.c = con\n",
    "    ur.r = rec\n",
    "    ur.gripperController = gripperController\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "else:\n",
    "    print(\"UR5 + Gripper Interface Established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.openGripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (2) - Hardware Test - Raises gripper 1 cm and open's closes gripper\n",
    "ur.testRoutine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (3) - Initialize connection to RealSense\n",
    "import RealSense as real\n",
    "real = real.RealSense()\n",
    "real.initConnection()\n",
    "# real.displayStream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (4) - RealSense and YOLO Initialization\n",
    "# robot_model steps can deleted once extrinsic-free transforms are verfied to be correct\n",
    "# robot_model = RTB_Model()\n",
    "# robot_model.setJointAngles(ur.getJointAngles())\n",
    "import ObjectDetection as ob\n",
    "try:\n",
    "    detector = ob.ObjectDetection(real,None,moveRelative = True)\n",
    "except Exception as e:\n",
    "    detector.real.pipe.stop()\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.openGripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (5) - Image Measurements, Segmentation, and Processing into Blocks\n",
    "# import Block as bl\n",
    "# urPose = ur.getPose()\n",
    "# pcd,rgbdImage = detector.real.getPCD()\n",
    "# depthImage,colorImage = rgbdImage.depth,rgbdImage.color\n",
    "# blocks = detector.getBlocksFromImages(colorImage,depthImage,urPose,display = True)\n",
    "# for block in blocks:\n",
    "#     print(f\"{block.name}:\")\n",
    "#     print(f\"CamFrameCoords: {block.camFrameCoords}\")\n",
    "#     print(f\"GripperFrameCoords: {block.gripperFrameCoords}\")\n",
    "#     print(f\"WorldFrameCoords: {block.worldFrameCoords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (4) - Displaying PCD\n",
    "# detector.displayWorld(pcd,blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ur.openGripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (5) - Task Planning\n",
    "\n",
    "import TaskPlanner as tp\n",
    "import Block as bl\n",
    "urPose = ur.getPose()\n",
    "pcd,rgbdImage = detector.real.getPCD()\n",
    "depthImage,colorImage = rgbdImage.depth,rgbdImage.color\n",
    "blocks = detector.getBlocksFromImages(colorImage,depthImage,urPose,display = True)\n",
    "\n",
    "planner = tp.TaskPlanner(blocks)\n",
    "goalDict = {\"on\":[(\"blueBlock\",\"yellowBlock\")]}\n",
    "steps = planner.generatePlan(goalDict)\n",
    "print(steps)\n",
    "for block in blocks:\n",
    "    print(f\"{block.name} - {list(block.gripperFrameCoords)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT STEP (6) - Grasping blocks at each position in steps, returning to start position, moving to release position, moving back to start, opening gripper\n",
    "sleepRate = 0.75\n",
    "def projectToWorldCoords(gripperFrameCoords):\n",
    "    # given a goal position in gripper coords returns the displacements from the current pose in world coords\n",
    "    xB,yB,zB = gripperFrameCoords\n",
    "    # subtract 0.165 from block position in gripper frame to account for gripper length\n",
    "    zB -= 0.155\n",
    "    currentPose = ur.getPose() #SE3 Object\n",
    "    # print(f\"Current Pose:\\n{currentPose*1000}\")\n",
    "    R = currentPose.R \n",
    "    pX,pY,pZ = tuple(currentPose.t)\n",
    "    # xB,yB,zB here is the block position in the gripper frame which is aligned with the optoforce frame\n",
    "    P_goal = np.matmul(R,np.array([xB,yB,zB]).T)  # relative position of the block in world coordinates\n",
    "    print(f\"P_goal:\\n{P_goal}\")\n",
    "    dX,dY,dZ = tuple(P_goal) # quantities and directions the the gripper frame should be incremented to be centered at the block \n",
    "    return dX,dY,dZ\n",
    "    \n",
    "def moveToBlock(blockPos):\n",
    "    # would be better if this was block object\n",
    "    # :blockPos is coord in gripper frame\n",
    "    dX,dY,dZ = projectToWorldCoords(blockPos) # goalPose in world coordinates\n",
    "    homePose = ur.getPose()\n",
    "    dZ  += 7/1000 # up 7 mm to avoid hitting lower block\n",
    "    goal1 = copy.deepcopy(homePose)\n",
    "    goal1.t[2] += dZ\n",
    "    ur.moveL(goal1)\n",
    "    time.sleep(sleepRate)\n",
    "    goal2 = goal1\n",
    "    goal2.t[0] += dX\n",
    "    goal2.t[1] += dY\n",
    "    ur.moveL(goal2)\n",
    "    time.sleep(sleepRate)\n",
    "    \n",
    "def moveBackFromBlock(homePose):    \n",
    "    currentPose = ur.getPose()\n",
    "    # Move up 3 mm to avoid raise block to prevent friction from toppling lower block\n",
    "    goal1 = copy.deepcopy(currentPose)\n",
    "    goal1.t[2] += 3/1000\n",
    "    ur.moveL(goal1)\n",
    "    time.sleep(sleepRate)\n",
    "    currentPose = ur.getPose()\n",
    "    dX,dY,dZ = tuple(homePose.t - currentPose.t)\n",
    "    # Move in the XY Plane then Z Axis\n",
    "    goal2 = copy.deepcopy(currentPose)\n",
    "    goal2.t[0] += dX\n",
    "    goal2.t[1] += dY\n",
    "    ur.moveL(goal2)\n",
    "    time.sleep(sleepRate)\n",
    "    # Move in Z Axis back to home\n",
    "    goal3 = copy.deepcopy(goal2)\n",
    "    goal3.t[2] += dZ\n",
    "    ur.moveL(goal3)\n",
    "    time.sleep(sleepRate)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spatialmath as sm\n",
    "import copy\n",
    "import time\n",
    "\n",
    "goalBlock = blocks[1]\n",
    "blockLength = 0.02\n",
    "releaseCoords = goalBlock.gripperFrameCoords + goalBlock.getWorldFrameVerticalInGripper(blockLength)\n",
    "verticalDist = 0.02\n",
    "gX,gY,gZ = tuple(goalBlock.urPose.t)\n",
    "res = np.matmul(goalBlock.urPose.R,(sm.SE3.Trans([gX,gY,gZ+verticalDist]).t - goalBlock.urPose.t))\n",
    "# print(f\"res: {projectToWorldCoords(res)} \")\n",
    "# ur.openGripper() # Open gripper\n",
    "# ur.testRoutine()\n",
    "homePose = ur.getPose()\n",
    "\n",
    "for step in steps:\n",
    "    # Grasp and Move Home Step\n",
    "    grabPos,releasePos = step\n",
    "    moveToBlock(grabPos) \n",
    "    print(\"Done moving to block\")\n",
    "    ur.closeGripper(9) \n",
    "    time.sleep(sleepRate)\n",
    "    moveBackFromBlock(homePose)\n",
    "    moveToBlock(releasePos)\n",
    "    ur.closeGripper(55)\n",
    "    moveBackFromBlock(homePose)\n",
    "    ur.openGripper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.c.disconnect()\n",
    "ur.r.disconnect()\n",
    "time.sleep(2)\n",
    "try:\n",
    "    robotIP = \"192.168.0.6\"\n",
    "    con = rtde_control.RTDEControlInterface(robotIP)\n",
    "    rec = rtde_receive.RTDEReceiveInterface(robotIP)\n",
    "    #ur = UR5_Interface() -- commented out since when imported it is defined as ur. if you run this before importing uncommnet --Jensen\n",
    "    ur.c = con\n",
    "    ur.r = rec\n",
    "    ur.gripperController = gripperController\n",
    "    time.sleep(5)\n",
    "    ur.testRoutine()\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "else:\n",
    "    print(\"UR5 + Gripper Interface Established\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "planner.locPositions[\"loc-a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.moveL(homePose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.openGripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ur.moveL(homePose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.openGripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAY WANT TO BREAK THIS UP INTO SEPARATE CELLS\n",
    "import UR5_Interface as ur \n",
    "def runRoutine():\n",
    "    print(\"1\")\n",
    "    try:\n",
    "        #ur = UR5_Interface()\n",
    "        ur.c = con\n",
    "        ur.r = rec\n",
    "        ur.gripperController = gripperController\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "        \n",
    "    print(\"2\")\n",
    "    # robot_model = RTB_Model()\n",
    "    # robot_model.setJointAngles(ur.getJointAngles())\n",
    "    # Set joint angles so object detection has correct extrinsics\n",
    "    \n",
    "    # Instantiates ObjectDetection object which intializes a connection to the realsense\n",
    "    detector = ObjectDetection(robot_model,moveRelative = True)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        ur.openGripper()\n",
    "        print(f\"Nth Frame Pose:\\n{np.array(ur.getPose())}\")\n",
    "        # Takes images for display\n",
    "        pcd,rgbdImage = detector.real.getPCD()\n",
    "        depthImage,colorImage = rgbdImage.depth,rgbdImage.color\n",
    "        # detector.real.displayImages(depthImage,colorImage)\n",
    "        \n",
    "        blocks = detector.getBlocksFromImages(colorImage,depthImage)\n",
    "        for block in blocks:\n",
    "            print(f\"{block.name}:\")\n",
    "            print(f\"CamFrameCoords: {block.camFrameCoords}\")\n",
    "            print(f\"GripperFrameCoords: {block.gripperFrameCoords}\")\n",
    "            print(f\"WorldFrameCoords: {block.worldFrameCoords}\")\n",
    "        m = MotionPlanner(blocks,moveRelative = True)\n",
    "        # Interface to the UR5\n",
    "        m.ur = ur \n",
    "        # m.rtb_model = robot_model\n",
    "        redBlock,yellowBlock,blueBlock = blocks\n",
    "        redPCD,yellowPCD,bluePCD = redBlock.blockPCD,yellowBlock.blockPCD,blueBlock.blockPCD\n",
    "        redAABB,yellowAABB,blueAABB = redBlock.blockAABB,yellowBlock.blockAABB,blueBlock.blockAABB,\n",
    "        detector.real.displayPCD([redPCD,yellowPCD,bluePCD,redAABB,yellowAABB,blueAABB])\n",
    "        m.runMovement()\n",
    "        # time.sleep(3)\n",
    "        # ur.closeGripper(10)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "    finally:\n",
    "        detector.real.pipe.stop()\n",
    "        # ur.c.disconnect()\n",
    "        # ur.r.disconnect()\n",
    "runRoutine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ur.c.disconnect()\n",
    "ur.r.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "robot_model = RTB_Model()\n",
    "robot_model.plotRobot()\n",
    "x,y,z = [],[],[]\n",
    "for point in redPCD.points:\n",
    "    x.append(point[0])\n",
    "    y.append(point[1])\n",
    "    z.append(point[2])\n",
    "\n",
    "print(np.mean(x))\n",
    "print(np.mean(y))\n",
    "print(np.mean(z))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(x,y,z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(real.extrinsics is None) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
